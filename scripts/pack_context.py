#!/usr/bin/env python3
"""ì§ˆë¬¸ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ íŒ© ìƒì„± ìŠ¤í¬ë¦½íŠ¸"""

import argparse
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

import frontmatter

# ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë£¨íŠ¸
ROOT = Path(__file__).parent.parent
VAULT_DIR = ROOT / "vault"
LOGS_DIR = ROOT / "logs"


def load_notes():
    """Vaultì˜ ëª¨ë“  ë…¸íŠ¸ ë¡œë“œ"""
    notes = []
    for filepath in VAULT_DIR.rglob("*.md"):
        try:
            post = frontmatter.load(filepath)  # type: ignore
            meta = post.metadata or {}
            notes.append(
                {
                    "path": filepath,
                    "rel_path": filepath.relative_to(ROOT),
                    "id": meta.get("id"),
                    "type": meta.get("type"),
                    "title": meta.get("title", filepath.stem),
                    "tags": meta.get("tags", []),
                    "links": meta.get("links", []),
                    "created": meta.get("created"),
                    "updated": meta.get("updated"),
                    "confidence": meta.get("confidence", "medium"),
                    "text": post.content,
                    "metadata": meta,
                }
            )
        except Exception as e:
            print(f"âš ï¸  íŒŒì‹± ì‹¤íŒ¨: {filepath} - {e}")
    return notes


def extract_section(text: str, header: str) -> str:
    """íŠ¹ì • ì„¹ì…˜ ì¶”ì¶œ (## Summary, ## Key Points ë“±)"""
    pattern = rf"^## {re.escape(header)}\s*\n(.*?)(?:\n## |\Z)"
    match = re.search(pattern, text, flags=re.S | re.M)
    if match:
        return match.group(1).strip()
    return ""


def score_note(note: dict[str, Any], query_terms: list[str]) -> float:
    """ë…¸íŠ¸ì˜ ì§ˆë¬¸ ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚°"""
    text = (note["title"] + "\n" + note["text"]).lower()
    score = 0.0
    # ì œëª© ë§¤ì¹˜ (ê°€ì¤‘ì¹˜ 3ë°°)
    title_lower = note["title"].lower()
    for term in query_terms:
        if term in title_lower:
            score += 3.0
    # ë³¸ë¬¸ ë§¤ì¹˜
    for term in query_terms:
        score += text.count(term)
    # íƒœê·¸ ë§¤ì¹˜ (ê°€ì¤‘ì¹˜ 2ë°°)
    tags_str = " ".join(note["tags"]).lower()
    for term in query_terms:
        if term in tags_str:
            score += 2.0
    return score


def expand_links(notes: list[dict], seed_ids: set[str], hops: int = 1) -> set[str]:
    """ì‹œë“œ ë…¸íŠ¸ë¡œë¶€í„° N-hop ë§í¬ í™•ì¥"""
    # IDë¡œ ë…¸íŠ¸ ì¸ë±ì‹±
    notes_by_id = {n["id"]: n for n in notes if n["id"]}
    current = seed_ids.copy()
    expanded = seed_ids.copy()
    for _ in range(hops):
        next_layer = set()
        for note_id in current:
            note = notes_by_id.get(note_id)
            if not note:
                continue
            # ë§í¬ íƒìƒ‰
            for link in note["links"]:
                target = link.get("to")
                if target and target not in expanded:
                    next_layer.add(target)
        expanded.update(next_layer)
        current = next_layer
        if not current:
            break
    return expanded


def get_recent_notes(notes: list[dict], days: int = 30) -> set[str]:
    """ìµœê·¼ Nì¼ ì´ë‚´ ì—…ë°ì´íŠ¸ëœ ë…¸íŠ¸ ID ìˆ˜ì§‘"""
    cutoff = datetime.now() - timedelta(days=days)
    recent = set()
    for note in notes:
        updated = note.get("updated")
        if not updated:
            continue
        try:
            # YAML date íŒŒì‹±
            if isinstance(updated, str):
                updated_dt = datetime.strptime(updated, "%Y-%m-%d")
            else:
                updated_dt = updated
            if updated_dt >= cutoff:
                if note["id"]:
                    recent.add(note["id"])
        except Exception:
            pass
    return recent


def create_context_pack(
    question: str,
    seed_ids: list[str] | None = None,
    hops: int = 1,
    recent_days: int = 30,
    topk: int = 10,
    max_tokens: int = 8000,
):
    """ì»¨í…ìŠ¤íŠ¸ íŒ© ìƒì„±"""
    print("ğŸ“š ë…¸íŠ¸ ë¡œë”© ì¤‘...")
    notes = load_notes()
    print(f"   ì´ {len(notes)}ê°œ ë…¸íŠ¸ ë¡œë“œë¨")
    # ì§ˆë¬¸ í‚¤ì›Œë“œ ì¶”ì¶œ
    query_terms = [
        t.lower() for t in re.findall(r"[A-Za-zê°€-í£0-9_]+", question) if len(t) >= 2
    ]
    print(f"ğŸ” í‚¤ì›Œë“œ: {', '.join(query_terms[:10])}")
    # í›„ë³´ ID ìˆ˜ì§‘
    candidate_ids = set()
    # (1) ì‹œë“œ ë…¸íŠ¸ + ë§í¬ í™•ì¥
    if seed_ids:
        seed_set = set(seed_ids)
        expanded = expand_links(notes, seed_set, hops)
        candidate_ids.update(expanded)
        print(f"ğŸ”— ì‹œë“œ í™•ì¥: {len(seed_set)} â†’ {len(expanded)}ê°œ")
    # (2) ìµœê·¼ ë…¸íŠ¸
    recent = get_recent_notes(notes, recent_days)
    candidate_ids.update(recent)
    print(f"ğŸ“… ìµœê·¼ {recent_days}ì¼: {len(recent)}ê°œ")
    # (3) í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ìƒìœ„ topk
    scored = [(n, score_note(n, query_terms)) for n in notes]
    scored.sort(key=lambda x: x[1], reverse=True)
    for note, score in scored[:topk]:
        if note["id"] and score > 0:
            candidate_ids.add(note["id"])
    # í›„ë³´ í•„í„°ë§ ë° ì •ë ¬
    notes_by_id = {n["id"]: n for n in notes if n["id"]}
    candidate_notes = [notes_by_id[nid] for nid in candidate_ids if nid in notes_by_id]
    # ì ìˆ˜ ì¬ê³„ì‚° ë° ì •ë ¬
    candidate_notes = [(n, score_note(n, query_terms)) for n in candidate_notes]
    candidate_notes.sort(key=lambda x: x[1], reverse=True)
    print(f"âœ… í›„ë³´: {len(candidate_notes)}ê°œ")
    # ì»¨í…ìŠ¤íŠ¸ íŒ© ìƒì„±
    output_lines = []
    output_lines.append("# CONTEXT PACK v1\n")
    output_lines.append("## Question\n")
    output_lines.append(f"{question}\n")
    output_lines.append("\n## Constraints\n")
    output_lines.append("- ë‹µë³€ì€ vault ìŠ¤í‚¤ë§ˆì— ë§ì¶°ì„œ ì•¡ì…˜/ê²°ì •/ë…¸íŠ¸ ë§í¬ê¹Œì§€ ì œì•ˆ")
    output_lines.append("- ê°€ëŠ¥í•˜ë©´ ê¸°ì¡´ ë…¸íŠ¸ë¥¼ ë§í¬í•˜ê³ , ìƒˆ ë…¸íŠ¸ê°€ í•„ìš”í•˜ë©´ ì œì•ˆ")
    output_lines.append("\n## Relevant Notes\n")
    total_tokens = 0
    included = 0
    for note, score in candidate_notes:
        # ì„¹ì…˜ ì¶”ì¶œ
        summary = extract_section(note["text"], "Summary")
        key_points = extract_section(note["text"], "Key Points")
        section_text = f"\n### [{note['id']}] {note['title']}\n"
        if summary:
            section_text += f"\n**Summary:**\n{summary}\n"
        if key_points:
            section_text += f"\n**Key Points:**\n{key_points}\n"
        section_text += f"\n- Type: {note['type']}"
        section_text += f"\n- Tags: {', '.join(note['tags'][:5])}"
        section_text += f"\n- Path: `{note['rel_path']}`"
        section_text += f"\n- Confidence: {note['confidence']}"
        # ë§í¬ ì •ë³´
        if note["links"]:
            links_str = ", ".join([f"`{link.get('to')}`" for link in note["links"][:5]])
            section_text += f"\n- Links: {links_str}"
        section_text += "\n"
        # í† í° ì²´í¬ (ëŒ€ëµ 1 í† í° = 4ì)
        est_tokens = len(section_text) // 4
        if total_tokens + est_tokens > max_tokens and included > 3:
            print(f"âš ï¸  í† í° ì œí•œ ë„ë‹¬ ({total_tokens} tokens)")
            break
        output_lines.append(section_text)
        total_tokens += est_tokens
        included += 1
    # ì¶œë ¥ íŒŒì¼ ì €ì¥
    output_text = "\n".join(output_lines)
    timestamp = datetime.now().strftime("%Y-%m-%d_%H%M%S")
    slug = re.sub(r"[^\wê°€-í£]+", "_", question[:30]).strip("_")
    output_path = LOGS_DIR / f"contextpack_{timestamp}_{slug}.md"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(output_text, encoding="utf-8")
    print(f"\nâœ… ìƒì„± ì™„ë£Œ: {output_path.relative_to(ROOT)}")
    print(f"   í¬í•¨ëœ ë…¸íŠ¸: {included}ê°œ")
    print(f"   ì˜ˆìƒ í† í°: ~{total_tokens}")
    return output_path


def main():
    parser = argparse.ArgumentParser(
        description="LLMìš© ì»¨í…ìŠ¤íŠ¸ íŒ© ìƒì„±ê¸°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  python pack_context.py "RAG ì‹œìŠ¤í…œ ì„¤ê³„ ë°©ë²•"
  python pack_context.py "alignment ì—°êµ¬ ë™í–¥" --seed topic.alignment --hops 2
  python pack_context.py "qraft í”„ë¡œì íŠ¸ í˜„í™©" --seed project.qraft --recent-days 7
        """,
    )
    parser.add_argument(
        "question",
        help="ì§ˆë¬¸ ë˜ëŠ” ì£¼ì œ",
    )
    parser.add_argument(
        "--seed",
        action="append",
        help="ì‹œë“œ ë…¸íŠ¸ ID (ì—¬ëŸ¬ ê°œ ì§€ì • ê°€ëŠ¥: --seed id1 --seed id2)",
    )
    parser.add_argument(
        "--hops",
        type=int,
        default=1,
        help="ë§í¬ í™•ì¥ ê¹Šì´ (ê¸°ë³¸: 1)",
    )
    parser.add_argument(
        "--recent-days",
        type=int,
        default=30,
        help="ìµœê·¼ Nì¼ ë…¸íŠ¸ í¬í•¨ (ê¸°ë³¸: 30)",
    )
    parser.add_argument(
        "--topk",
        type=int,
        default=10,
        help="í‚¤ì›Œë“œ ë§¤ì¹­ ìƒìœ„ Nê°œ (ê¸°ë³¸: 10)",
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=8000,
        help="ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸: 8000)",
    )
    args = parser.parse_args()
    create_context_pack(
        question=args.question,
        seed_ids=args.seed,
        hops=args.hops,
        recent_days=args.recent_days,
        topk=args.topk,
        max_tokens=args.max_tokens,
    )


if __name__ == "__main__":
    main()
